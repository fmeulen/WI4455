---
title: An exercise on comparing maximum likelihood, Bayes and empirical Bayes
author: CS4070 - WI4455 (Frank van der Meulen - Delft University of Technology)
date: November 2020
---

In this exercise we consider the model
```math
  X_i \mid \Theta_i = \theta_i \stackrel{ind}{\sim} Unif(0,\theta_i)\\
  \Theta_1,\ldots, \Theta_n \stackrel{iid}{\sim} Ga(2,\lambda)
```
Here $\lambda$ will initially be considered as a hyperparameter (that is, fixed -- known).
Note that the classical (frequentist) model does not involve a prior and is given by
```math
X_1,\ldots, X_n \stackrel{iid}{\sim} Unif(0\theta_i)
```
It is easily verified that the mle for $\theta_i$ is given by $x_i$ (just make a sketch of the likelihood function).
Here we compare the mle with various estimators derived from the Bayesian setup.
For this model, one can show that conditional on $X_1,\ldots, X_n$, the random variables $\Theta_1,\ldots, \Theta_n$ are independent and that
$\Theta_i \mid X_i \sim X_i + Z_i$, where $\{Z_i,\, 1\le i\le n\} \stackrel{iid}{\sim} Exp(\lambda)$, independent of all other random variables.
In particular, the posterior mean satisfies $E[\Theta_i \mid X_i] = X_i + 1/\lambda$.
Furthermore, one can show that the empirical Bayes estimate (obtained by maximum likelihood type II) for $\lambda$
equals $1/\bar{X}_n$.


To study the estimators numerically, we f irst load some packages.
```julia
using DataFrames
using LinearAlgebra
using Distributions
using Cairo
using Gadfly
```

Let's generate some artifical data to work with. In the following `n` will denote the sample size.
The "true" $\theta$ values are generated by drawing independently from the uniform distribution on $[0,1000]$. So here we go
```julia
n = 100
θ₀ = rand(Uniform(0,1000.0),n);
```
So here are the true values displayed
```julia
Gadfly.plot(x=collect(1:n), y=θ₀, Guide.xlabel("i"), Guide.ylabel("θᵢ"))
```
Now the data are obtained from
```julia
x = [rand(Uniform(0,θ₀[i])) for i ∈ eachindex(θ₀)];
```
Note that I use the `Gadfly` plotting package here. It is somewhat similar to ``ggplot2` in `R`.
The estimators are easily computed
```julia
mle = x;
ebayes = x .+ mean(x);
λ = 0.010 # hyperpar in prior
bayes = x .+ 1.0/λ;
```
Now the performance of the three estimators can be compared by computed the sum of the squared deviations between the estimates and true values:
```julia
ss(x) = sum(x.^2)
D = DataFrame(method = ["mle", "bayes", "ebayes"],
      ss = [ss(mle-θ₀),ss(bayes-θ₀),ss(ebayes-θ₀)])
print(D)
```
To make the comparison slightly easier, let's add a column, taking mle as benchmark.
```julia
D[!,:ss_rel] = D[!,:ss]./D[1,:ss];
print(D)
```
So it is definitely worthwhile considering empirical Bayes here!. The performance of the truly Bayesian method where the posterior mean depends on $\lambda$
is of course hugely dependent on the choice of this parameter.
Clearly, if $\lambda$ is large, the posterior mean and mle will  be about the same.
Finally, we visualise the estimators
```julia
d = DataFrame(i=repeat(1:n,outer=3),
              deviation = vcat(mle - θ₀, bayes - θ₀, ebayes - θ₀),
              type = repeat(["mle", "bayes", "ebayes"], inner=n))
p = Gadfly.plot(d, x=:i, y=:deviation, color=:type, xgroup=:type,
      yintercept=[0.0],
      Geom.subplot_grid(Geom.point, Geom.hline ))
```


## Fully Bayesian approach by employing a prior on $\lambda$

Instead of empirical Bayes, we can add an additional layer to the Bayesian model specification. Such a thing is wise when it is noticed
that the posterior is much influenced by the choice of a hyperparameter ($\lambda$ in this case).
Let's assume a slightly different model, where $\lambda$ is not a hyperparameter, but  $\lambda \sim \mbox{Ga}(\alpha, \beta)$.
```math
  X_i \mid \Theta_i = \theta_i \stackrel{ind}{\sim} Unif(0,\theta_i)\\
  \Theta_1,\ldots, \Theta_n \mid \Lambda=\lambda \stackrel{iid}{\sim} Ga(2,\lambda)\\
  \Lambda \sim Ga(\alpha, \beta)
```
So now $\alpha$ and $\beta$ are hyperparameters in the model.

```julia
α = 1.0
β = 1.0
```
One can take other values later. For now notice that the prior mean of $\lambda$ equals $\alpha/ \beta =1$.
We run a Gibbs sampler, in which the $\theta_i$ and $\lambda$ are updated in separate steps.
The steps are quite easy:
```julia
IT = 10000  # number of iterations
λ = [α/β]   # initialise with prior mean
θ = zeros(IT,n) # each row contains an MCMC iteration
for it in 2:IT
    θ[it,:] = x .+ rand(Exponential(1/λ[it-1]))
    β_ = β + sum(θ[it,:])
    push!(λ, rand(Gamma(2*n + α, 1.0/β_)))
end
```
[In `julia` the Exponential and Gamma distributions use a parametrisation in terms of the scale parameter, not the rate parameter. One should
always be careful in this.]
Let's first check that the chain has *converged* to its stationary region (one can never make the claim with certainty, but we can check
      if there are obvious deviations from stationarity). So we make a trace plot for $\lambda$.
```julia
Gadfly.plot(x=1:IT, y = λ, Geom.path,
      Guide.xlabel("i"), Guide.ylabel("λ[i]"))
```
This looks perfectly fine! The chain reaches its stationary regime almost instantly.
Also note that apparently we learn about the value of $\lambda$. Whereas a priori it was $1.0$, aposteriori
it is on average about $0.005$. This is not too far from $1/\bar{x}_n \approx 0.0043$.
Keep that in mind when rereading this post and comparing empirical Bayes with full Bayes using a prior on $\lambda$.

To check the chain, also trace plots for all $\theta_i$ could be made... However, there are many parameters, so we are not going to check all.
Let's consider the third parameter
```julia
Gadfly.plot(x=1:IT, y=θ[:,3], Geom.path,
      Guide.xlabel("i"), Guide.ylabel("θ₃[i]"))
```
The posterior distribution for this coefficient seems skewed, so taking a log-transformation may tidy up the traceplot a bit.
```julia
Gadfly.plot(x=1:IT, y=log.(θ[:,3]), Geom.path,
      Guide.xlabel("i"), Guide.ylabel("log(θ₃[i])"))
```
```julia
log(x[3])
```
The printed number should make you understand why there are no posterior samples smaller than $6.5$.

We consider the first half of the iterations as burnin, and compute the average of the remainder of the samples.
```julia
its = div(IT,2):IT
θ̂ = [mean(x[its]) for x ∈ eachcol(θ)]
λ̂ = mean(λ[its])
```
For plotting purposes, we add the results of $\hat\theta$ to the dataframe `d`.
```julia
d_ = DataFrame(i=1:n, deviation= θ̂ - θ₀, type=fill("hierbayes",n))
append!(d,d_);
```
Next, we can redo the plotting
```julia
p = Gadfly.plot(d, x=:i, y=:deviation, color=:type, xgroup=:type,
      yintercept=[0.0],
      Geom.subplot_grid(Geom.point, Geom.hline ))
```
It is clear that the fully Bayesian approach and empirical Bayes give similar point estimates.
Now the Bayesian approach gives full uncertainty quantification. For example, for $\theta_1$ we can make
a histogram of the posterior samples
```julia
lθ1 = log.(θ[:,1])
Gadfly.plot(x=lθ1, Geom.histogram, Guide.xlabel("log(θ₁)"), Guide.ylabel(""))
```
